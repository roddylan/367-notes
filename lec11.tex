\documentclass{article}
\usepackage{textcomp, gensymb}
\usepackage{utf8add}
\usepackage[most]{tcolorbox}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{systeme}
\usepackage{tcolorbox}
\usepackage{outlines}
\usepackage{bbm}
\usepackage[ruled,longend]{algorithm2e}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\hypersetup{
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=black
}


\title{CMPUT 367}
\author{Roderick Lan}
\date{}

\usepackage{natbib}
\makeatletter
% \crefformat{tcb@cnt@Example}{example~#2#1#3}
% \Crefformat{tcb@cnt@Example}{Example~#2#1#3}
\makeatother
\newtcbtheorem[auto counter, number within = subsection]
{definition}{Definition}{%                                                        
  breakable,
  fonttitle = \bfseries,
  colframe = blue!75!black,
  colback = blue!10
}{def}


\makeatother
\newtcbtheorem[auto counter, number within = subsection]
{example}{Example}{%                                                        
  breakable,
  fonttitle = \bfseries,
  colframe = orange!75!black,
  colback = orange!10
}{ex}

\makeatother
\newtcbtheorem[auto counter, number within = subsection]
{thm}{Thm}{%                                                        
  breakable,
  fonttitle = \bfseries,
  colframe = orange!75!black,
  colback = orange!10
}{thm}

\makeatother
\newtcbtheorem[auto counter, number within = subsection]
{expln}{Explain}{%                                                        
  breakable,
  fonttitle = \bfseries,
  colframe = red!75!black,
  colback = red!10
}{exp}

\makeatother
\newtcbtheorem[auto counter, number within = section]
{proof}{Proof}{%                                                        
  breakable,
  fonttitle = \bfseries,
  colframe = gray!75!black,
  colback = gray!10
}{prf}


\begin{document}

\maketitle

\tableofcontents
\break

\section*{Lecture 11 - Feb 15}
% \noindent\rule{\textwidth}{0.5pt}
\section{Nonlinear Problems}
(last lecture)
Stack linear classifiers; use another linear classifier to classify obj based on results of stack
\\
Weights/learnable params are $w$ and $b$ for each linear (sigmoid) classifier

\begin{expln}
{How to get weights?}{}
Manually "program" the weights
\\or \\
Use principled ML method to discover weights (ML training method)
\end{expln}
\noindent
Problem is NON-CONVEX (big problem in Deep Learning)
\\
Symmetry of weights to prove non-convexity
\\
Use Gradient Descent to find local minimas; 
improve minimas by performing Gradient Descent multiple times
\begin{algorithm}
    \caption{GD update}
    Loop:\\[5pt]
    $
    w^{(new)} \gets w ^{(old)} - \alpha \frac{\partial J}{\partial w} \big | _{w=w^{(old)}}
    $
\end{algorithm}

\begin{flalign*}
    h &= \sigma (w^\top c + b) = \frac{1}{1+e^{-(w_1c_1 + w_2c_2 + b)}}\\
    &= \frac{1}{1+
    e^{-\left (
        w_1 \frac{1}{1+e^{-(w_{11}x_1 + w_{12}x_2 + b_1)}} 
        +
        w_2 \frac{1}{1+e^{-(w_{21}x_1 + w_{22}x_2 + b_2)}} 
        + b
    \right )}
    }
\end{flalign*}
Differentiable
\[
    \frac{\partial J}{\partial w_{11}} = \frac{\partial J}{\partial h} \cdot
    \frac{\partial h}{\partial w_{11}}
\]
Need a way to systematically organize the derivatives in a neat way to gain \textbf{insight}
\section{Neural Network}
Building block is a Neuron; takes $d-$dimensional input 
\[
    \mathbf x \to z \to y=f(z)
\]
$\mathbf x = \begin{bmatrix}
    x_1 \\ \vdots \\ x_d
\end{bmatrix}$
,
$f$ is the activation function (ie. sigmoid, ReLU, $\tanh$, etc.)
\\[5pt]
% Gradient for ReLU doesn't give usefull information; not differentiable
ReLU most common
\\[5pt]
Arbitrary stacking of neurons is NOT convenient for both mathematically calculating
derivatives 
\\[5pt]
Laywise fully connected NN
\\[5pt]
Output of neuron is continuous 
\\
Balance width and height of neural net






























































\end{document}