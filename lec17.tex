\documentclass{article}
\usepackage{textcomp, gensymb}
\usepackage{utf8add}
\usepackage[most]{tcolorbox}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{systeme}
\usepackage{tcolorbox}
\usepackage{outlines}
\usepackage{bbm}
\usepackage[ruled,longend]{algorithm2e}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\hypersetup{
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=black
}


\title{CMPUT 367}
\author{Roderick Lan}
\date{}

\usepackage{natbib}
\makeatletter
% \crefformat{tcb@cnt@Example}{example~#2#1#3}
% \Crefformat{tcb@cnt@Example}{Example~#2#1#3}
\makeatother
\newtcbtheorem[auto counter, number within = subsection]
{definition}{Definition}{%                                                        
  breakable,
  fonttitle = \bfseries,
  colframe = blue!75!black,
  colback = blue!10
}{def}


\makeatother
\newtcbtheorem[auto counter, number within = subsection]
{example}{Example}{%                                                        
  breakable,
  fonttitle = \bfseries,
  colframe = orange!75!black,
  colback = orange!10
}{ex}

\makeatother
\newtcbtheorem[auto counter, number within = subsection]
{thm}{Thm}{%                                                        
  breakable,
  fonttitle = \bfseries,
  colframe = orange!75!black,
  colback = orange!10
}{thm}

\makeatother
\newtcbtheorem[auto counter, number within = subsection]
{expln}{Explain}{%                                                        
  breakable,
  fonttitle = \bfseries,
  colframe = red!75!black,
  colback = red!10
}{exp}

\makeatother
\newtcbtheorem[auto counter, number within = section]
{proof}{Proof}{%                                                        
  breakable,
  fonttitle = \bfseries,
  colframe = gray!75!black,
  colback = gray!10
}{prf}


\begin{document}

\maketitle

\tableofcontents
\break

\section*{Lecture 17 - Mar 19}
Prior knowledge abt gradient
\\
(finish - first 9 min)


\subsection{Prior Knowledge}
CNN - spatial neighborhood; sliding window
\\
RNN - ordered info; sequential proc
\\
LLM has momentum, variance, gradient info
\\[10pt]
Don't want to compress weight, want to compress gradient information without losing information
\begin{list}{}{}
    \item pytorch and tf give gradient of all weights (whole memory space)
    \item can also do it layer by layer
\end{list}
\noindent
grad = loss.back()
\\
parm = opt.adam(param, grad)


\section{Structured Network}
Has parse tree structure info.

\subsection{Recursive Propagation}
RNN that incorporates parse structure 


\section{Tree Based Convolution}
Tree structure instead of sequence



\section{Seq2Seq and Attention}
\subsubsection{Sidenote:}
In compilers, CFG is NP-complete.
\\
Avoid backtracking, look forward languages preferred

\subsection{Seq2Seq}
Given sequence of input signals (usually time series info. like text)
\\
Encoder / Decoder structure
\\
(finish; 41 min in lec)
\\
\textbf{Important:} Need to feed data back

\subsubsection{Training}
Decoder input layer:
\begin{list}{}{}
    \item Attempt 1 - feed in predicted
    \item Attempt 2 - feed in ground truth
\end{list}
Cannot feed ground ground truth during inference

\subsubsection{Inference}
Decoder input layer - feed in predicted words
\\
Terminate w/ EOS token (EOS predicted $\to$ sentence terminated)

\subsubsection{Caveat}
Batch implementation
\begin{list}{}{}
    \item Padding EOS or 0 vector $\to$ incorrect
    \item Masking $\to$ correct; 1 = actual step, 0 = virtual step
\end{list}

\subsubsection{Inference Criteria}
Single output classification
\begin{list}{}{}
    \item MAP inference $\iff$ minimal empirical loss 
    \item (get most likely category given inference)
    \item \[
        y = \argmax p(y | x)
    \]
\end{list}
Sentence Generation
\begin{list}{}{}
    \item generate best sentence
    \[
        \mathbf y = \argmax_\mathbf y p(\mathbf{y | x})
    \]
    \item arg max over entire sentence space
    \item greedy can sometimes work
    \item exhaustive search too long ($O(b^l)$)
\end{list}
Use something in between greedy and exhaustive $\to$ beam search
\begin{list}{}{}
    \item try all words or a few best
    \item dont maintain all; maintian only several good partial seq. 
\end{list}

\subsection{Beam Search}


\subsection{Isuses with Autoreg}
Error accumulation
\begin{list}{}{}
    \item fix by feed in self gen words and anneal ground truth
\end{list}
% \\[5pt]
Label bias
\begin{list}{}{}
    \item Locally normalized models prefer highly probable (but possibly unimportant)
    words in a more conentrated pred.
\end{list}








\end{document}