\documentclass{article}
\usepackage{textcomp, gensymb}
\usepackage{utf8add}
\usepackage[most]{tcolorbox}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{systeme}
\usepackage{tcolorbox}
\usepackage{outlines}
\usepackage[ruled,longend]{algorithm2e}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\hypersetup{
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=black
}


\title{CMPUT 367}
\author{Roderick Lan}
\date{}

\usepackage{natbib}
\makeatletter
% \crefformat{tcb@cnt@Example}{example~#2#1#3}
% \Crefformat{tcb@cnt@Example}{Example~#2#1#3}
\makeatother
\newtcbtheorem[auto counter, number within = subsection]
{definition}{Definition}{%                                                        
  breakable,
  fonttitle = \bfseries,
  colframe = blue!75!black,
  colback = blue!10
}{def}

\makeatother
\newtcbtheorem[auto counter, number within = subsection]
{example}{Example}{%                                                        
  breakable,
  fonttitle = \bfseries,
  colframe = orange!75!black,
  colback = orange!10
}{ex}

\makeatother
\newtcbtheorem[auto counter, number within = subsection]
{expln}{Explain}{%                                                        
  breakable,
  fonttitle = \bfseries,
  colframe = red!75!black,
  colback = red!10
}{exp}

\makeatother
\newtcbtheorem[auto counter, number within = section]
{proof}{Proof}{%                                                        
  breakable,
  fonttitle = \bfseries,
  colframe = gray!75!black,
  colback = gray!10
}{prf}


\begin{document}

\maketitle

\tableofcontents
\break

\section{Lecture 4 - Jan 23}
\subsection{Review from prev lec}
Classification:
\begin{list}{}{}
    \item Logistic regression
        \begin{flalign*}
            \mathbf x^{(m)} &\in \mathbb R ^d \\
            t^{(m)} &\in \{0, 1\}
        \end{flalign*}
    \item Scoring Function: (not in input space)
        \[
            z^{(m)} = \mathbf w^\top \mathbf x^{(m)} + b
        \]
        Linear/affine function used because it is simple; have to specify 
        \\human/function preference. \\[3pt]
        Can also build nonlinear models\\
        Design non linear features (ie. $x_2 = x^2$) $\to$ model no longer linear in terms of raw output, but function still linear
        \\
        Kernel method = uses nonlinear inner product
        \\
        Stacking multiple (learnable) linear classifiers (basis of NN)
        \\[3pt]
        Complex models can reduce to simpler models \\
        Linear models are the basis for all ML

        
    \item Linear scoring function $\to$ sigmoid function ('squish' it to have range (0,1))
        \begin{definition}
            {Sigmoid Function}{}
            \[ 
                y^{(m)} = \sigma (z^{(m)}) = \frac{1}{1+e^{-z^{(m)}}} = \frac{1}{1+e^{
                    -\mathbf w^\top \mathbf x ^{(m)} +b
                }}
            \]
            "goodness" function; no negative $\to$ "badness" function
        \end{definition}
        $\mathrm{Pr} [t^{(m)} = 1 | x^{(m)}] = y^{(m)}$
        \\
        $\mathrm{Pr} [t^{(m)} = 0 | x^{(m)}] = 1 - y^{(m)}$\\
        $\mathcal D_{\mathrm{train}} = \{ (x^{(m)} , t^{(m)})_{m=1}^M$
    \item Likelihood:
        \[
            p(\mathcal D_{\mathrm{train}}; w,b) = \prod_{m=1}^{M} y^{(m)t^{(m)}}
            (1-y^{(m)})^{1-t^{(m)}}
        \]
    \item Loss: (CE loss)
        \[
            J(w,b) = \frac{1}{M} \sum_{m=1}^{M} \left [
                -t^{(m)}\ln y^{(m)} - (1-t^{(m)})\ln (1-y^{(m)})
            \right ]
        \]
\end{list}
\noindent

\subsection{Multiclass Classification}
(softmax)\\
$\mathbf x^{(m)} \in \mathbb R^d$\\
$t^{(m)} \in \{ 1, 2, \ldots, k \}$
\\
Need multiple scoring functions for multiple classes; \\
Convenient to have scoring function for each class\\
$z_1^{(m)} = \mathbf w_1^\top \mathbf x^{(m)} + b_1, \cdots, \mathbf w_k^\top \mathbf x^{(m)} + b_k$
\\
$k$ scoring functions for convenience, but one is redundant
\begin{definition}
    {Categorical Distribution}{}
    \[
        \mathrm{Cat}(\pi_1, \cdots, \pi_k) \ \ \ \pi_k \ge 0 \ \forall k
    \]
    Probability of $k = \pi_k$
    % \\
    \[ \sum_{k}^{} \pi_k = 1 \]
\end{definition}
\noindent
use $\mathrm{exp}$ because it is montonically increasing and unbounded; maps $\mathbb R \to \mathbb R_{++}$
\[
    T = \sum_{k=1}^{k} \exp (\mathbf w_k^\top \mathbf x^{(m)} + b_k) \text{ for normalizing}
\]
\[
    y^{(1)} = \frac{\exp (\mathbf w_1^\top \mathbf x^{(m)} + b_1)}{T}, \cdots, \frac{\exp (\mathbf w_k^\top \mathbf x^{(m)} + b_k)}{T} = y^{(k)}
\]
\noindent
Range of function should be $(0, +\infty)$
\begin{quote}
    "Why does it have to be positively unbounded, what would happen if bounded?" 
    Can't tell the difference b/w two similar samples
\end{quote}

\[
    \mathrm{Pr}[t^{(m)} = k | x^{(m)} ] = y^{(m)}_k
\]
$
\mathcal D _{\mathrm{train}} = \{ ( x^{(m)}, t^{(m)} ) \}_{m=1}^M
$\\[3pt]
Likelihood: 
\[
    p(\mathcal D_{\mathrm{train}}; \{ w_k, b_k \}_{k=1}^K  ) 
    = \prod_{m=1}^{M} { y_1^{(m)} }^{\mathbb I\{ t^{(m)}=1 \}} 
    { y_2^{(m)} }^{\mathbb I\{ t^{(m)}=2 \}} \cdots
    { y_k^{(m)} }^{\mathbb I\{ t^{(m)}=k \}}
\]
\[
    t_k^{(m)} = \mathbb I \{t^{(m)} = k \} 
\]
\[
    p(\mathcal D_{\mathrm{train}}; \{ w_k, b_k \}_{k=1}^K  ) 
    = \prod_{m=1}^{M} { y_1^{(m)} }^{t_1^{(m)}} 
    { y_2^{(m)} }^{t_2^{(m)}} \cdots
    { y_k^{(m)} }^{t_k^{(m)}}
\]
\noindent
Log-Likelihood:
\begin{flalign*}
    \ln p(\mathcal D_{\mathrm{train}}) &= \sum_{m=1}^{M} \ln { y_1^{(m)} }^{t_1^{(m)}} \cdots { y_k^{(m)} }^{t_k^{(m)}} \\
    &= \sum_{m=1}^{M} \left [
        t_1^{(m)}\ln y_1^{(m)} + \cdots + t_k^{(m)}\ln y_k^{(m)}
    \right ]\\
    &= \sum_{m=1}^{M}\sum_{k=1}^{K}t_k^{(m)} \ln y^{(m)}_k
\end{flalign*}
\noindent
Loss:
\begin{flalign*}
    J &= -\sum_{m=1}^{M}\sum_{k=1}^{K}t_k^{(m)} \ln y^{(m)}_k \\
    &= -\sum_{m=1}^{M}\ln y^{(m)}_{t^{(m)}} 
    \ \ \ \ \ \ \ \ \text{  $t^{(m)}$ serves as index}
\end{flalign*}
$t^{(m)} \in \{ 1,\ldots, k \}$ index representation
\[
    \mathbf t^{(m)}=
    \begin{bmatrix}
        t_1^{(m)} \\
        \vdots \\
        t_k^{(m)}
    \end{bmatrix}=
    \begin{bmatrix}
        \mathbb I \{t^{(m)} = 1 \}\\
        \vdots \\
        \mathbb I \{t^{(m)} = k \}
    \end{bmatrix} \ \ \ \ \ \ \ \text{one-hot representation}
\]
\noindent
$\hat t_* = \argmax_k = y_k^*$\\[3pt]
$y_k^* = \mathrm{Pr} [t^{*} =k | x^*]$\\[5pt]
{[ Max A Posteriori Inference ]}
\\[10pt]
Questions:
\begin{quote}
    For softmax, what if $k=2$? \\
    Optimization for log. regression and softmax?
\end{quote}


\end{document}